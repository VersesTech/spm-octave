function [MDP] = spm_MDP_VB_XXX(MDP,OPTIONS)
% active inference and learning using belief propagation (factorised)
% FORMAT [MDP] = spm_MDP_VB_XXX(MDP,OPTIONS)
%
% Input; MDP(m,n)       - structure array of m models over n epochs
% MDP.U(1,F)            - controllable factors
% MDP.T                 - number of outcomes
%
% MDP.A{G}(O,N1,...,NF) - likelihood of O outcomes for modality G, given hidden states
% MDP.B{F}(N,N,U)       - transitions among N states under U control states
% MDP.C{G}(O,1)         - prior probabilities over outcomes       (log preferences)
% MDP.D{F}(N,1)         - prior probabilities over initial states (Dirichlet counts)
% MDP.E{F}(U,1)         - prior probabilities over paths          (Dirichlet counts)
% MDP.H{F}(N,1)         - prior probabilities over final states   (Dirichlet counts)
%
% MDP.a{G}              - concentration parameters for A
% MDP.b{F}              - concentration parameters for B
% MDP.c{G}              - concentration parameters for C
% MDP.d{F}              - concentration parameters for D
% MDP.e{P}              - concentration parameters for E
% MDP.h{P}              - concentration parameters for H
%
% optional:
% MDP.s(F,T)            - true states   - for each hidden factor
% MDP.o(G,T)            - true outcomes - for each outcome modality
% MDP.O{G,T}            - likelihoods   - for each outcome modality
% MDP.u(F,T)            - true controls - for each hidden factor
%
% MDP.alpha             - precision - action selection [512]
% MDP.beta              - precision - active learning [0]
% MDP.chi               - Occams window for deep updates
% MDP.eta               - Forgetting hyperparameter [128]
% MDP.N                 - depth of deep policy search [N = 0]
% MDP.k(1,F)            - beliefs about controllable factors
%
% MDP.demi.C            - Mixed model: cell array of true causes (DEM.C)
% MDP.demi.U            - Bayesian model average (DEM.U) see: spm_MDP_DEM
% MDP.link              - link array to generate outcomes from
%                         subordinate MDP for deep (hierarchical) models
%
% MDP.n(O,T)            - outputs for modality O at time T are generated by
%                         agent n(O,T); unless n(O,T) = 0, when outputs
%                         are generated by the agents states.
%                         If n(O,T) < 0, outputs are generated by all
%                         agents
% MDP.m(F)              - states for factor F are generated from agent m(F);
%                         unless m(F) = 0, when states are updated for the
%                         agent in question
%
% OPTIONS.A             - switch to evaluate explicit action
% OPTIONS.B             - switch to evaluate backwards pass (replay)
% OPTIONS.N             - switch to evaluate neuronal responses
% OPTIONS.P             - switch to plot graphics: [default: 0]
% OPTIONS.D             - switch to update initial states with final states
% OPTIONS.BMR           - Bayesian model reduction for multiple trials
%                         see: spm_MDP_VB_sleep(MDP,BMR)
% Outputs:
%
% MDP.P{F}(U,T)         - conditional expectations over hidden paths
% MDP.X{F}(N,T)         - conditional expectations over hidden states
% MDP.Y{O,T}            - conditional expectations over outcomes
% MDP.R(P,T)            - conditional expectations over policies
%
% MDP.F(1,T)            - (negative) free energies (states)  over time
% MDP.Z{U,T}            - (negative) free energies (control) over time
% MDP.G{P,T}            - (negative) expected free energies  over time
% MDP.Fa(G)             - (negative) free energy of parameters (a)
% MDP.Fb(F)             - ...
%
% MDP.v                 - expected free energy over policies
% MDP.w                 - precision of beliefs about policies
% MDP.un                - simulated neuronal encoding of hidden states
% MDP.xn                - simulated neuronal encoding of policies
% MDP.wn                - simulated neuronal encoding of precision (tonic)
% MDP.dn                - simulated dopamine responses (phasic)
%
% This routine provides solutions of active inference (minimisation of
% variational free energy) using a generative model based upon a Markov
% decision process. The model and inference scheme is formulated in
% discrete space and time. This means that the generative model (and
% process) are hidden Markov models whose dynamics are given by transition
% probabilities among states and the likelihood corresponds to a particular
% outcome conditioned upon hidden states.
%
% This implementation equips agents with the prior beliefs that they will
% maximise expected free energy. Expected free energy can be interpreted in
% several ways - most intuitively as minimising the KL divergence between
% predicted and preferred outcomes (specified as prior beliefs) -i.e., risk
% while simultaneously minimising ambiguity. Alternatively, this can be
% rearranged into expected information gain and expected value, where value
% is the log of prior preferences (overstates or outcomes).
%
% This implementation generalises previous MDP based formulations of active
% inference by equipping each factor of latent states with a number of
% paths; some of which may be controllable and others not. Controllable
% factors are now specified with indicator variables in the vector MDP.U.
% Furthermore, because the scheme uses sophisticated inference (i.e., a
% recursive tree search accumulating path integral is of expected free
% energy) a policy reduces to a particular combination of controllable
% paths or dynamics over factors. In consequence, posterior beliefs cover
% latent states and paths; with their associated variational free energies.
% Furthermore, it is now necessary to specify the initial states and the
% initial paths using D and E respectively. In other words, he now plays
% the role of a prior over the path of each factor that can only be changed
% if it is controllable (it no longer corresponds to a prior over
% policies).
%
% In addition to state and path estimation (and policy selection), the
% scheme also updates model parameters; including the state transition
% matrices, mapping to outcomes and the initial state. This is useful for
% learning the context. Likelihood and prior probabilities can be specified
% in terms of concentration parameters (of a Dirichlet distribution
% (a,b,c,...). If the corresponding (A,B,C,...) are supplied, they will be
% used to generate outcomes.
%
% This scheme allows for differences in the functional form of priors –
% specified in terms of probability transition tensors – between the
% generating process and generative model. The generative model is, by
% default, specified in terms of Dirichlet parameters, while the generative
% process is specified in terms of expected (likelihood and prior
% transition) probabilities: b and B, respectively. If the number or
% dimensionality of b and B do not correspond, then select OPTIONS.A = 1.
% This will automatically evaluate the most likely policy (combination of
% controllable paths) to reproduce the predicted outcomes (i.e. that which
% minimises variational free energy or maximises accuracy); as opposed to
% using the path selected by the model.
%
% scheme is designed for any allowable policies or control variables
% specified in MDP.U. Constraints on allowable policies can limit the
% numerics or combinatorics considerably. Further, the outcome space and
% hidden states can be defined in terms of factors; corresponding to
% sensory modalities and (functionally) segregated representations,
% respectively. This means, for each factor or subset of hidden states
% there are corresponding control states that determine the transition
% probabilities. in this implementation, hidden factors are combined using
% a Kronecker intensive product to enable exact Bayesian inference using
% belief propagation (the Kronecker tensor form ensures that conditional
% dependencies among hidden factors are evaluated).
%
% In this belief propagation scheme, the next action is evaluated in terms
% of the free energy expected under all subsequent actions until some time
% horizon (specified by MDP.T). This expected free energy is accumulated
% along all allowable paths or policies (see the subroutine spm_forward);
% effectively, performing a deep tree search over future sequences of
% actions. Because actions are conditionally independent of previous
% actions, it is only necessary to update posterior beliefs over hidden
% states at the current time point (using a Bayesian belief updating) and
% then use the prior over actions (based upon expected free energy) to
% select the next action. Previous actions are inferred under the posterior
% beliefs over current states; i.e., inferred state transitions.
%
% In brief, the agent encodes beliefs about hidden states in the past
% conditioned on realised outcomes and actions. The resulting conditional
% expectations determine the (path integral) of free energy that then
% determines an empirical prior over the next action, from which the next
% realised action sampled
%
%
% If supplied with a structure array, this routine will automatically step
% through the implicit sequence of epochs (implicit in the number of
% columns of the array). If the array has multiple rows, each row will be
% treated as a separate model or agent. This enables agents to communicate
% through acting upon a common set of hidden factors, or indeed sharing the
% same outcomes.
%
% See also: spm_MDP, which uses multiple future states and a mean field
% approximation for control states - but allows for different actions at
% all times (as in control problems).
%
% See also: spm_MDP_VB_X,  which is the corresponding variational message
% passing scheme for fixed policies; i.e., ordered sequences of actions
% that are specified a priori.
%
% See also: spm_MDP_VB_XX,  which is the corresponding variational message
% passing scheme for sophisticated policy searches under the assumption
% that the generative process and model have the same structure
%
%
%__________________________________________________________________________
% Copyright (C) 2019 Wellcome Trust Centre for Neuroimaging

% Karl Friston
% $Id: spm_MDP_VB_XXX.m 8448 2023-07-27 21:20:42Z karl $


% options
%--------------------------------------------------------------------------
try, OPTIONS.A; catch, OPTIONS.A = 0; end      % action selection
try, OPTIONS.B; catch, OPTIONS.B = 0; end      % backwards pass
try, OPTIONS.D; catch, OPTIONS.D = 0; end      % final states
try, OPTIONS.N; catch, OPTIONS.N = 0; end      % neuronal responses
try, OPTIONS.P; catch, OPTIONS.P = 0; end      % graphics

% check MDP specification
%--------------------------------------------------------------------------
MDP = spm_MDP_checkX(MDP);

% set up and preliminaries
%==========================================================================

% defaults
%--------------------------------------------------------------------------
try, alpha = MDP(1).alpha; catch, alpha = 512;  end % action  precision
try, beta  = MDP(1).beta ; catch, beta  = 0;    end % learing precision
try, eta   = MDP(1).eta;   catch, eta   = 512;  end % learnability
try, chi   = MDP(1).chi;   catch, chi   = 1/64; end % Occam window updates
try, N     = MDP(1).N;     catch, N     = 0;    end % depth of policy search
    
% initialise model-specific parameters
%==========================================================================
T     = MDP(1).T;                              % number of updates
Nm    = numel(MDP);
for m = 1:Nm

    % number of outcomes, states, controls and policies
    %----------------------------------------------------------------------
    Ng(m) = numel(MDP(m).A);                   % number of outcome factors
    Nf(m) = numel(MDP(m).B);                   % number of hidden factors
    for g = 1:Ng(m)
        No(m,g) = size(MDP(m).A{g},1);         % number of outcomes
    end
    for f = 1:Nf(m)
        Ns(m,f) = size(MDP(m).B{f},1);         % number of hidden states
        Nu(m,f) = size(MDP(m).B{f},3);         % number of hidden controls
    end

    % allow for differences between the model (b) and process (B)
    %----------------------------------------------------------------------
    NF    = Nf;
    NS    = Ns;
    NU    = Nu;
    if isfield(MDP(m),'b')
        Nf(m) = numel(MDP(m).b);               % number of hidden factors
        for f = 1:Nf(m)
            Ns(m,f) = size(MDP(m).b{f},1);     % number of hidden states
            Nu(m,f) = size(MDP(m).b{f},3);     % number of hidden controls
        end
    end

    % parameters of generative model and policies
    %======================================================================

    % likelihood model (for a partially observed MDP)
    %----------------------------------------------------------------------
    for g = 1:Ng(m)

        % parameters (concentration parameters): a
        %------------------------------------------------------------------
        if isfield(MDP,'a')
            a{m,g} = MDP(m).a{g};
        else
            a{m,g} = MDP(m).A{g}*512;
        end

        % normalised likelihood
        %------------------------------------------------------------------
        A{m,g} = spm_norm(a{m,g});

        % prior concentration parameters and novelty (W)
        %------------------------------------------------------------------
        if isfield(MDP,'a')
            W{m,g} = spm_wnorm(a{m,g});
        else
            W{m,g} = false;
        end

        % and ambiguity (w) (for computation of expected free energy: G)
        %------------------------------------------------------------------
        if islogical(a{m,g})
            K{m,g} = false;
        else
            K{m,g} = spm_hnorm(a{m,g});
        end

    end

    % transition probabilities (priors)
    %----------------------------------------------------------------------
    for f = 1:Nf(m)

        % parameters (concentration parameters): b
        %--------------------------------------------------------------
        if isfield(MDP,'b')
            b{m,f} = MDP(m).b{f};
        else
            b{m,f} = MDP(m).B{f}*512;
        end

        % normalised transition probabilities
        %------------------------------------------------------------------
        B{m,f}     = spm_norm(b{m,f});

        % prior concentration parameters and novelty (I)
        %------------------------------------------------------------------
        if isfield(MDP,'b')
            I{m,f} = spm_wnorm(b{m,f});
        else
            I{m,f} = false;
        end

        % priors over initial hidden states: concentration parameters
        %------------------------------------------------------------------
        if isfield(MDP,'d')
            d{m,f} = MDP(m).d{f};
        elseif isfield(MDP,'D')
            d{m,f} = MDP(m).D{f}*512;
        else
            d{m,f} = ones(Ns(m,f),1);
        end

        % normalised prior probabilities
        %------------------------------------------------------------------
        D{m,f}     = spm_norm(d{m,f});

        % priors over final hidden states: concentration parameters
        %------------------------------------------------------------------
        if isfield(MDP,'h')
            h{m,f} = MDP(m).h{f};
        elseif isfield(MDP,'H')
            h{m,f} = MDP(m).H{f}*512;
        else
            h{m,f} = ones(Ns(m,f),1);
        end

        % normalised prior probabilities
        %------------------------------------------------------------------
        H{m,f}     = spm_norm(h{m,f});


        % priors over paths (control states): concentration parameters
        %------------------------------------------------------------------
        if isfield(MDP,'e')
            e{m,f} = MDP(m).e{f};
        elseif isfield(MDP,'E')
            e{m,f} = MDP(m).E{f}*512;
        else
            e{m,f} = ones(Nu(m,f),1);
        end

        % normalised prior probabilities
        %------------------------------------------------------------------
        E{m,f}     = spm_norm(e{m,f});

    end


    % prior preferences over outcomes (log probabilities) : C
    %----------------------------------------------------------------------
    for g = 1:Ng(m)

        if isfield(MDP,'c')
            c{m,g} = MDP(m).c{g};
        elseif isfield(MDP,'C')
            c{m,g} = MDP(m).C{g};
        else
            c{m,g} = zeros(No(m,g),1);
        end

        % prior preferences (log probabilities) : C
        %------------------------------------------------------------------
        C{m,g} = spm_log(spm_softmax(c{m,g}));

    end

    % allowable actions (U)
    %----------------------------------------------------------------------
    U{m}      = any(MDP(m).U,1);                % controllable factors

    % actual policies (J)
    %----------------------------------------------------------------------
    if OPTIONS.A
        k         = find(U{m});
        u         = spm_combinations(NU(m,k));
        J{m}      = zeros(size(u,1),NF(m));
        J{m}(:,k) = u;
    end

    % controllable factors
    %----------------------------------------------------------------------
    if isfield(MDP(m),'k')
        U{m} = any(MDP(m).k,1);
    end

    % beliefs about policies (V)
    %----------------------------------------------------------------------
    if size(MDP(m).U,1) == 1
        k         = find(U{m});
        u         = spm_combinations(Nu(m,k));
        V{m}      = zeros(size(u,1),Nf(m));
        V{m}(:,k) = u;
    else
        V{m} = MDP(m).U;
    end
    Np(m)    = size(V{m},1);                  % number of policies

    
    % initialise posterior expectations of hidden states (X) and paths (S)
    %======================================================================
    for f = 1:Nf(m)
        for t = 1:T
            Q{m,f,t} = D{m,f};
        end
        X{m,f} = repmat(D{m,f},1,T);
        S{m,f} = repmat(E{m,f},1,T);

        if OPTIONS.N
            sn{m,f} = zeros(Ns(m,f),T,T) + 1/Ns(m,f);
        end
    end

    % initialise posteriors over control states
    %----------------------------------------------------------------------
    for f = 1:Nf(m)
        for t = 1:T
            P{m,f,t} = E{m,f};
        end
    end

    % if states have not been specified, set to 0
    %----------------------------------------------------------------------
    k        = zeros(NF(m),T);
    try
        i    = find(MDP(m).s);
        k(i) = MDP(m).s(i);
    end
    MDP(m).s = k;

    % if paths have not been specified, set to 0
    %----------------------------------------------------------------------
    k        = zeros(NF(m),T);
    try
        i    = find(MDP(m).u);
        k(i) = MDP(m).u(i);
    end
    MDP(m).u = k;

    % if outcomes have not been specified set to 0
    %======================================================================
    k        = zeros(Ng(m),T);
    try
        i    = find(MDP(m).o);
        k(i) = MDP(m).o(i);
    end
    MDP(m).o = k;

    % if outcomes are specified probabilistically
    %----------------------------------------------------------------------
    for g  = 1:Ng(m)
        for t = 1:T
            try
                % Probabilistic outcomes
                %----------------------------------------------------------
                O{m,g,t}      = MDP(m).O{g,t};

                % Overwrite deterministic outcomes
                %----------------------------------------------------------
                MDP(m).o(g,t) = find(rand < cumsum(O{m,g,t}),1);

            catch
                O{m,g,t}      = [];
            end
        end
    end


    % domains (id)
    %----------------------------------------------------------------------
    id{m} = MDP(m).id;

end % end model (m)

% ensure any outcome generating agent is updated first
%--------------------------------------------------------------------------
T       = max(T);                            % maximum number of updates
N       = min(N,T);                          % depth of policy search
[M,MDP] = spm_MDP_get_M(MDP,T,Ng);           % order of model updating

% belief updating over successive time points
%==========================================================================
for t = 1:T

    % generate hidden states and controls for each agent or model
    %======================================================================
    for m = M(t,:)

        % initialise and propagate control state (path)
        %==================================================================
        for f = 1:Nf(m)
            if ~MDP(m).u(f,t)
                if t > 1

                    % previous path
                    %------------------------------------------------------
                    MDP(m).u(f,t) = MDP(m).u(f,t - 1);

                else

                    % otherwise sample a path
                    %------------------------------------------------------
                    pu            = spm_norm(MDP(m).E{f});
                    MDP(m).u(f,t) = find(rand < cumsum(pu),1);

                end
            end
        end

        % action generating outcomes (for controllable factors)
        %==================================================================
        if t > 1

            % implcit action
            %--------------------------------------------------------------
            for f = 1:Nf(m)

                if U{m}(f)
                    pu                = P{m,f,t - 1};
                    u                 = find(rand < cumsum(pu),1);
                    MDP(m).u(f,t - 1) = u;

                    % prior postdictive density (paths)
                    %------------------------------------------------------
                    P{m,f,t - 1}(:)   = 0;
                    P{m,f,t - 1}(u)   = 1;

                end

                % prior predictive density over hidden states
                %==========================================================
                Q{m,f,t} = spm_dot(B{m,f},P(m,f,t - 1))*Q{m,f,t - 1};

            end


            % explicit action
            %--------------------------------------------------------------
            if OPTIONS.A
  
                % predicted outcomes
                %----------------------------------------------------------
                for g = 1:Ng(m)

                    % domain of A{g}
                    %------------------------------------------------------
                    j     = id{m}.A{g};
                    qo{g} = spm_dot(A{m,g},Q(m,j,t));

                end

                % find actions that minimise prediction error
                %----------------------------------------------------------
                i = any(J{m},1);
                for k = 1:size(J{m},1)

                    % predicted states under this policy
                    %------------------------------------------------------
                    u     = MDP(m).u(:,t - 1);
                    u(i)  = J{m}(k,i);
                    for f = 1:numel(MDP(m).B)
                        qs{f} = MDP(m).B{f}(:,MDP(m).s(f,t - 1),u(f));
                    end

                    % free energy or prediction error (i.e., inaccuracy)
                    %------------------------------------------------------
                    F(k)  = 0;
                    for g = 1:Ng(m)
                        po   = spm_dot(MDP(m).A{g},qs);
                        F(k) = F(k) - qo{g}'*spm_log(po);
                    end
                end

                % most likely control state
                %----------------------------------------------------------
                [F,k] = min(F);
                MDP(m).u(i,t - 1) = spm_vec(J{m}(k,i));

            end

        end % end generating paths or actions


        % sample state if not specified
        %==================================================================
        for f = 1:NF(m)
            if ~MDP(m).s(f,t)

                if t > 1

                    % the next state is generated by state transititions
                    %----------------------------------------------------------
                    ps = MDP(m).B{f}(:,MDP(m).s(f,t - 1),MDP(m).u(f,t - 1));

                else

                    % unless it is the initial state
                    %------------------------------------------------------
                    ps = spm_norm(MDP(m).D{f});

                end

                MDP(m).s(f,t) = find(rand < cumsum(ps),1);

            end

        end % end generating states

    end % end generating over models


    % share states if specified in MDP.m
    %----------------------------------------------------------------------
    for m = M(t,:)
        for f = 1:NF(m)
            if isfield(MDP(m),'m')
                n = MDP(m).m(f);
                if n
                    MDP(m).s(f,t) = MDP(n).s(f,t);
                end
            end
        end
    end

    % generate outcomes O{m,g,t} for each agent or model
    %======================================================================
    for m = M(t,:)

        % sample outcome, if not specified
        %------------------------------------------------------------------
        for g = 1:Ng(m)

            % domain of A{g}
            %--------------------------------------------------------------
            j  = id{m}.A{g};

            % if outcome is not specified
            %--------------------------------------------------------------
            if ~MDP(m).o(g,t)

                % outcome is generated by model n
                %----------------------------------------------------------
                if MDP(m).n(g,t) > 0

                    n    = MDP(m).n(g,t);
                    if n == m

                        % outcome that maximises accuracy (i.e. ELBO)
                        %--------------------------------------------------
                        F             = spm_log(spm_dot(A{m,g},Q(m,j,t)));
                        O{m,g,t}      = spm_softmax(F*512);
                        MDP(m).o(g,t) = find(rand < cumsum(O{m,g,t}),1);

                    else

                        % outcome from model n (previously sampled)
                        %--------------------------------------------------
                        O{m,g,t}      = O{n,g,t};
                        MDP(m).o(g,t) = MDP(n).o(g,t);

                    end

                elseif MDP(m).n(g,t) < 0

                    % outcome is generated by all models or agents
                    %------------------------------------------------------
                    Fm{g,m} = spm_log(spm_dot(A{m,g},Q(m,j,t)));

                else

                    % or default sample from likelihood, given hidden state
                    %======================================================
                    ind           = MDP(m).s(:,t);
                    ind           = num2cell(ind(j));
                    O{m,g,t}      = MDP(m).A{g}(:,ind{:});
                    MDP(m).o(g,t) = find(rand < cumsum(O{m,g,t}),1);

                end
                
            else

                % use sampled outcomes if no probabilistic specification
                %----------------------------------------------------------
                if isempty(O{m,g,t})
                    O{m,g,t} = full(sparse(MDP(m).o(g,t),1,1,No(m,g),1));
                end
            end
        end
    end


    % shared probabilistic outcomes O{m,g,t} and realizatons MDP(m).o(g,t)
    %======================================================================
    for m = M(t,:)
        for g = 1:Ng(m)

            % share outcomes if specified in MDP.m
            %--------------------------------------------------------------
            if MDP(m).n(g,t) < 0
                j = 1:Nm; j(m) = []; % sensory attenuation
         
                F             = sum([Fm{g,j}],2);
                O{m,g,t}      = spm_softmax(F);
                po            = spm_softmax(F*512);
                MDP(m).o(g,t) = find(rand < cumsum(po),1);

            end
        end
    end


    % or generate outcomes O{m,g,t} from a subordinate MDP
    %======================================================================
    for m = M(t,:)

        if isfield(MDP,'link')

            % use previous inversions (if available) to generate outcomes
            %--------------------------------------------------------------
            try
                mdp = MDP(m).mdp(t);
            catch
                try
                    mdp = spm_MDP_update(MDP(m).MDP(t),MDP(m).mdp(t - 1));
                catch
                    try
                        mdp = spm_MDP_update(MDP(m).MDP(1),MDP(m).mdp(t - 1));
                    catch
                        mdp = MDP(m).MDP(1);
                    end
                end
            end

            % priors over states (of subordinate level)
            %--------------------------------------------------------------
            mdp.factor = [];
            for f = 1:size(MDP(m).link,1)
                for g = 1:size(MDP(m).link,2)
                    if ~isempty(MDP(m).link{f,g})

                        % subordinate factor has hierarchical constraints
                        %--------------------------------------------------
                        mdp.factor(end + 1) = f;

                        % empirical priors over initial states
                        %--------------------------------------------------
                        O{m,g,t} = spm_dot(A{m,g},Q(m,:,t));
                        mdp.D{f} = MDP(m).link{f,g}*O{m,g,t};

                        % outcomes (i.e., states) are generated by model n
                        %--------------------------------------------------
                        if MDP(m).n(g,t)
                            n    = MDP(m).n(g,t);
                            if m == n
                                ps         = MDP(m).link{f,g}(:,MDP(m).o(g,t));
                                mdp.s(f,1) = find(ps);
                            else
                                mdp.s(f,1) = MDP(n).mdp(t).s(f,1);
                            end
                        end

                        % hidden state for lower level is the outcome
                        %--------------------------------------------------
                        try
                            mdp.s(f,1) = mdp.s(f,1);
                        catch
                            ps         = MDP(m).link{f,g}(:,MDP(m).o(g,t));
                            mdp.s(f,1) = find(ps);
                        end

                    end
                end
            end


            % empirical prior preferences
            %--------------------------------------------------------------
            if isfield(MDP,'linkC')
                for f = 1:size(MDP(m).linkC,1)
                    for g = 1:size(MDP(m).linkC,2)
                        if ~isempty(MDP(m).linkC{f,g})
                            O{m,g,t} = spm_dot(A{m,g},Q(m,:,t));
                            mdp.C{f} = spm_log(MDP(m).linkC{f,g}*O{m,g,t});
                        end
                    end
                end
            end

            % empirical priors over policies
            %--------------------------------------------------------------
            if isfield(MDP,'linkE')
                mdp.factorE = [];
                for g = 1:size(MDP(m).linkE,2)
                    if ~isempty(MDP(m).linkE{g})
                        O{m,g,t} = spm_dot(A{m,g},Q(m,:,t));
                        mdp.E    = MDP(m).linkE{g}*O{m,g,t};
                    end
                end
            end


            % infer hidden states at lower level (outcomes at this level)
            %==============================================================
            OPT.B         = 1;
            MDP(m).mdp(t) = spm_MDP_VB_XXX(mdp,OPT);


            % get inferred outcomes from subordinate MDP
            %==============================================================
            for f = 1:size(MDP(m).link,1)
                for g = 1:size(MDP(m).link,2)
                    if ~isempty(MDP(m).link{f,g})
                        O{m,g,t} = MDP(m).link{f,g}'*MDP(m).mdp(t).X{f}(:,1);
                    end
                end
            end

            % if hierarchical preferences, these contribute to outcomes ...
            %--------------------------------------------------------------
            if isfield(MDP,'linkC')
                for f = 1:size(MDP(m).linkC,1)
                    for g = 1:size(MDP(m).linkC,2)
                        if ~isempty(MDP(m).linkC{f,g})
                            indC     = sparse(MDP(m).mdp(t).o(f,:)',1:length(MDP(m).mdp(t).o(f,:)),ones(length(MDP(m).mdp(t).o(f,:)),1),size(MDP(m).mdp(t).C{f},1),size(MDP(m).mdp(t).C{f},2));
                            O{m,g,t} = spm_softmax(spm_log(O{m,g,t}) + MDP(m).linkC{f,g}'*sum((indC.*(MDP(m).mdp(t).C{f})),2));
                        end
                    end
                end
            end

            % ... and the same for policies
            %--------------------------------------------------------------
            if isfield(MDP,'linkE')
                for g = 1:size(MDP(m).linkE,2)
                    if ~isempty(MDP(m).linkE{g})
                        O{m,g,t} = spm_softmax(spm_log(O{m,g,t}) + spm_log(MDP(m).linkE{g}'*MDP(m).mdp(t).R(:,end)));
                    end
                end
            end

            % ensure DEM starts with final states from previous inversion
            %--------------------------------------------------------------
            if isfield(MDP(m).MDP,'demi')
                MDP(m).MDP.DEM.G(1).x = MDP(m).mdp(t).dem(end).pU.x{1}(:,end);
                MDP(m).MDP.DEM.M(1).x = MDP(m).mdp(t).dem(end).qU.x{1}(:,end);
            end

        end % end of hierarchical mode (link)


        % or generate outcome likelihoods from a variational filter
        %==================================================================
        if isfield(MDP,'demi')

            % use previous inversions (if available)
            %--------------------------------------------------------------
            try
                MDP(m).dem(t) = spm_ADEM_update(MDP(m).dem(t - 1));
            catch
                MDP(m).dem(t) = MDP(m).DEM;
            end

            % get prior over outcomes
            %--------------------------------------------------------------
            for g = 1:Ng(m)
                O{m,g,t} = spm_dot(A{m,g},Q(m,:,t));
            end

            % get posterior outcome from Bayesian filtering
            %--------------------------------------------------------------
            MDP(m).dem(t) = spm_MDP_DEM(MDP(m).dem(t),...
                MDP(m).demi,O(m,:,t),MDP(m).o(:,t));

            for g = 1:Ng(m)
                O{m,g,t} = MDP(m).dem(t).X{g}(:,end);
            end

        end % end outcomes from Bayesian filter

    end % end loop over models or agents


    % Bayesian belief updating hidden states (Q) and controls (P)
    %======================================================================
    for m = M(t,:)

        % belief propagation (BP) under policy k
        %------------------------------------------------------------------
        for f = 1:Nf(m)
            for k = 1:Np(m)
                if V{m}(k,f)

                    % transitions and novelty for this policy
                    %------------------------------------------------------
                    BP{m,f,k} = B{m,f}(:,:,V{m}(k,f));
                    if any(I{m,f}(:))
                        IP{m,f,k} = I{m,f}(:,:,V{m}(k,f));
                    else
                        IP{m,f,k} = false;
                    end

                else

                    % transitions and novelty for this policy
                    %------------------------------------------------------
                    BP{m,f,k} = spm_dot(B{m,f},P(m,f,t));
                    if any(I{m,f}(:))
                        IP{m,f,k} = spm_dot(I{m,f},P(m,f,t));
                    else
                        IP{m,f,k} = false;
                    end

                end
            end
        end


        % posterior over hidden states (Q) and expected free energy (G)
        %==================================================================
        [G,Q,F] = spm_forwards(O,Q,A,BP,C,H,K,W,IP,t,T,min(T,t + N),m,id);

        % augment with prior probability over paths
        %------------------------------------------------------------------
        if t == 1
            for k = 1:Np(m)
                LE     = 0;
                for f  = find(U{m})
                    LE = LE + spm_log(E{m,f}(V{m}(k,f)));
                end
                G(k)   = G(k) + LE;
            end
        end

        % prior beliefs about policies (R) and precision (w)
        %------------------------------------------------------------------
        R{m}(:,t)  = spm_softmax(G);
        w{m}(t)    = R{m}(:,t)'*spm_log(R{m}(:,t));
        v{m}(t)    = R{m}(:,t)'*G;


        % posterior over previous path (P)
        %==================================================================
        Z  = 0;
        if t > 1
            for f = 1:Nf(m)

                % log likelihood of paths
                %----------------------------------------------------------
                LL = spm_dot(spm_dot(B{m,f},Q{m,f,t}),Q{m,f,t - 1});
                LL = spm_log(LL);

                % prior over previous path
                %----------------------------------------------------------
                LP = spm_log(P{m,f,t - 1});

                % posterior over previous path = prior over current path
                %----------------------------------------------------------
                P{m,f,t - 1} = spm_softmax(LL + LP);

                % (negative) complexity of paths (or control states)
                %----------------------------------------------------------
                Z  = Z + P{m,f,t - 1}'*(LL + LP - spm_log(P{m,f,t - 1}));

            end

        end % end previous control states


        % prior over current path or control state (P)
        %==================================================================

        % prior over policy (i.e.,action combination)
        %------------------------------------------------------------------
        Pu    = spm_softmax(alpha*G);
        for f = 1:Nf(m)
            
            if U{m}(f)

                % prior over control state
                %----------------------------------------------------------
                for u = 1:Nu(m,f)
                    P{m,f,t}(u) = Pu'*(V{m}(:,f) == u);
                end

            else

                % or unchanging path
                %----------------------------------------------------------
                if t > 1
                    P{m,f,t} = P{m,f,t - 1};
                end
            end

        end % end current control states


        % active (likelihood) learning
        %==================================================================

        % mapping from hidden states to outcomes: a
        %------------------------------------------------------------------
        if isfield(MDP(m),'a') && any(U{m})
            for g = 1:Ng(m)

                % domain of A{g}
                %----------------------------------------------------------
                j      = id{m}.A{g};
                da     = spm_cross(O(m,g,t),Q{m,j,t});
                da     = da.*(a{m,g} > 0);

                % update likelihood Dirichlet parameters
                %----------------------------------------------------------
                a{m,g} = a{m,g} + da;
                A{m,g} = spm_norm(a{m,g});

                % prior concentration parameters and novelty (W)
                %----------------------------------------------------------
                W{m,g} = spm_wnorm(a{m,g});

                % and ambiguity (w)
                %----------------------------------------------------------
                K{m,g} = spm_hnorm(a{m,g});

            end
        end

        % mapping among hidden states: b
        %------------------------------------------------------------------
        if isfield(MDP(m),'b') && any(U{m}) && t > 1
            for f = 1:Nf(m)

                % domain of B{f}
                %----------------------------------------------------------
                db = spm_cross(spm_cross(Q{m,f,t},Q{m,f,t - 1}),P{m,f,t - 1});
                db = db.*(MDP(m).b{f} > 0);

                % update prior Dirichlet parameters
                %----------------------------------------------------------
                b{m,f} = b{m,f} + db;
                B{m,f} = spm_norm(b{m,f});

                % prior concentration parameters and novelty (W)
                %----------------------------------------------------------
                I{m,f} = spm_wnorm(b{m,f});

            end
        end


        % (ELBO) free energy: states, policies and paths
        %------------------------------------------------------------------
        MDP(m).F(t) = F;
        MDP(m).G{t} = G;
        MDP(m).Z(t) = Z;

        % save marginal posteriors over hidden states: c.f., working memory
        %==================================================================
        if OPTIONS.N
            for f = 1:Nf(m)
                for i = 1:T
                    sn{m,f}(:,i,t) = Q{m,f,i};
                end
            end
        end

        % return to supraordinate level
        %==================================================================

        % check for residual uncertainty (in hierarchical schemes)
        %------------------------------------------------------------------
        if isfield(MDP,'factor')

            % break if there is no further uncertainty to resolve
            %--------------------------------------------------------------
            for f = MDP(m).factor(:)'
                sq(m,f) = Q{m,f,t}'*spm_log(Q{m,f,t});
            end
            if sum(sq(:)) > - chi
                T = t;
            end
        end


    end % end of loop over models (agents)

    % terminate evidence accumulation
    %----------------------------------------------------------------------
    if t == T
        for m = 1:size(MDP,1)
            MDP(m).o  = MDP(m).o(:,1:T);        % outcomes at 1,...,T
            MDP(m).s  = MDP(m).s(:,1:T);        % states   at 1,...,T
            MDP(m).u  = MDP(m).u(:,1:T);        % control  at 1,...,T - 1
        end
    end

end 


% loop over models to accumulate Dirichlet parameters and prepare outputs
%==========================================================================
for m = 1:size(MDP,1)


    % Smoothing or backwards pass: replay
    %======================================================================
    if OPTIONS.B

        % Smoothing of unchanging control states
        %----------------------------------------------------------------------
        for f = 1:Nf(m)
            if ~U{m}(f)
                for t = 1:T
                    P{m,f,t} = P{m,f,T};
                end
            end
        end

        % reset parameters (concentration parameters): a
        %------------------------------------------------------------------
        for g = 1:Ng(m)
            if isfield(MDP,'a')
                a{m,g} = MDP(m).a{g};
            end
        end

        % reset parameters (concentration parameters): b
        %------------------------------------------------------------------
        for f = 1:Nf(m)
            if isfield(MDP,'b')
                b{m,f} = MDP(m).b{f};
            end
        end
        
        [Q,P,F]  = spm_backwards(O,P,Q,D,E,a,b,U,m,id);

        % update free energy
        %------------------------------------------------------------------
        MDP(m).F = F;

    end


    % prior Dirichlet parameters
    %======================================================================

    % over likelihood
    %----------------------------------------------------------------------
    for g = 1:Ng(m)
        if isfield(MDP,'a')
            pA{m,g} = MDP(m).a{g};
        end
        if isfield(MDP,'c')
            pC{m,g} = MDP(m).c{g};
        end
    end

    %  over hidden states
    %----------------------------------------------------------------------
    for f = 1:Nf(m)
        if isfield(MDP,'b')
            pB{m,f} = MDP(m).b{f};
        end
        if isfield(MDP,'d')
            pD{m,f} = MDP(m).d{f};
        end
        if isfield(MDP,'e')
            pE{m,f} = MDP(m).e{f};
        end
    end


    % learning - accumulate concentration parameters
    %======================================================================

    % likelihood mapping from hidden states to outcomes: a
    %------------------------------------------------------------------
    if isfield(MDP(m),'a')
        for g = 1:Ng(m)
            da = 0;
            for t = 1:T
                j  = id{m}.A{g};
                da = da + spm_cross(O(m,g,t),Q{m,j,t});

            end
            da = da.*(MDP(m).a{g} > 0);

            % active learning (based on expected free energy)
            %--------------------------------------------------------------
            if beta
                Fa(1,1)     = spm_MDP_MI(MDP(m).a{g},C{m,g});
                Fa(2,1)     = spm_MDP_MI(MDP(m).a{g} + da,C{m,g});
                Pa          = spm_softmax(beta*Fa);
                MDP(m).a{g} = (MDP(m).a{g} + Pa(2)*da)*eta/(eta + Pa(2));
            else
                MDP(m).a{g} = (MDP(m).a{g} + da)*eta/(eta + 1);
            end

        end
    end

    % mapping from hidden states to hidden states: b(u)
    %----------------------------------------------------------------------
    if isfield(MDP,'b')
        for f = 1:Nf(m)
            db = 0;
            for t = 1:(T - 1)
                db = db + spm_cross(spm_cross(Q{m,f,t + 1},Q{m,f,t}),P{m,f,t});
            end
            db = db.*(MDP(m).b{f} > 0);

            % active learning (based on expected free energy)
            %--------------------------------------------------------------
            if beta
                Fa(1,1)     = spm_MDP_MI(MDP(m).b{f});
                Fa(2,1)     = spm_MDP_MI(MDP(m).b{f} + db);
                Pa          = spm_softmax(beta*Fa);
                MDP(m).b{f} = (MDP(m).b{f} + Pa(2)*db)*eta/(eta + Pa(2));
            else
                MDP(m).b{f} = (MDP(m).b{f} + db)*eta/(eta + 1);
            end

        end
    end

    % accumulation of prior preferences: (c)
    %----------------------------------------------------------------------
    if isfield(MDP,'c') && t < T
        for g = 1:Ng(m)
            dc = O{m,g,t + 1};
            dc = dc.*(MDP(m).c{g} > 0);
            MDP(m).c{g} = (MDP(m).c{g} + dc)*eta/(eta + 1);
        end
    end

    % initial hidden states:
    %----------------------------------------------------------------------
    if isfield(MDP,'d')
        for f = 1:Nf(m)
            dd = Q{m,f,1};
            dd = dd.*(MDP(m).d{f} > 0);
            MDP(m).d{f} = (MDP(m).d{f} + dd)*eta/(eta + 1);
        end
    end

    % initial hidden states:
    %----------------------------------------------------------------------
    if isfield(MDP,'e')
        for f = 1:Nf(m)
            de = P{m,f,1};
            de = de.*(MDP(m).e{f} > 0);
            MDP(m).e{f} = (MDP(m).e{f} + de)*eta/(eta + 1);
        end
    end

    % (negative) free energy of parameters (complexity): outcome specific
    %======================================================================
    for g = 1:Ng(m)
        if isfield(MDP,'a')
            MDP(m).Fa(g) = - spm_KL_dir(MDP(m).a{g},pA{m,g});
        end
        if isfield(MDP,'c')
            MDP(m).Fc(f) = - spm_KL_dir(MDP(m).c{g},pC{g});
        end
    end

    % (negative) free energy of parameters: state specific
    %----------------------------------------------------------------------
    for f = 1:Nf(m)
        if isfield(MDP,'b')
            MDP(m).Fb(f) = - spm_KL_dir(MDP(m).b{f},pB{m,f});
        end
        if isfield(MDP,'d')
            MDP(m).Fd(f) = - spm_KL_dir(MDP(m).d{f},pD{m,f});
        end
        if isfield(MDP,'e')
            MDP(m).Fe(f) = - spm_KL_dir(MDP(m).e{f},pE{m,f});
        end
    end

    % posterior predictive density Y{g,t}
    %======================================================================
    for g = 1:Ng(m)
        for t = 1:T
            j = id{m}.A{g};
            MDP(m).Y{g,t} = spm_dot(A{m,g},Q(m,j,t));
        end
    end

    % reorganise posteriors for saving
    %======================================================================

    % states and paths
    %----------------------------------------------------------------------
    for t = 1:T
        for f = 1:Nf(m)
            X{m,f}(:,t) = Q{m,f,t};
            S{m,f}(:,t) = P{m,f,t};
        end
    end


    % assemble results and place in NDP structure
    %======================================================================
    MDP(m).T  = T;            % number of outcomes
    MDP(m).U  = V{m};         % policies
    MDP(m).R  = R{m};         % conditional expectations over policies
    MDP(m).C  = C(m,:);       % utility
    MDP(m).X  = X(m,:);       % conditional expectations over states
    MDP(m).P  = S(m,:);       % conditional expectations over paths
    MDP(m).O  = O(m,:,:);     % outcomes
    MDP(m).v  = v{m};         % expected free energy  over policies
    MDP(m).w  = w{m};         % precision of beliefs about policies

    MDP(m).O  = shiftdim(MDP(m).O,1);
    MDP(m).P  = shiftdim(MDP(m).P,1);

end % end loop over models (m)

end



function A  = spm_log(A)
    % log of numeric array plus a small constant
    %--------------------------------------------------------------------------
    A           = max(log(A),-32);
end

function A  = spm_norm(A)
    % normalisation of a probability transition matrix (columns)
    %--------------------------------------------------------------------------
    A           = rdivide(A,sum(A,1));
    A(isnan(A)) = 1/size(A,1);
end 


function A  = spm_wnorm(A)
    % expected information gain (likelihood parameters)
    % A = minus(log(A0),log(A)) + minus(1./A,1./A0) + minus(psi(A),psi(A0))
    %   = minus(1./A,1./A0)/2 + ...
    %--------------------------------------------------------------------------
    A   = max(A,1/32);
    if max(A(:)) < 256
        A0  = sum(A);
        A   = minus(log(A0),log(A)) + minus(1./A,1./A0) + minus(psi(A),psi(A0));
        A   = max(A,0);
    else
        A   = false;
    end
end
    
function A  = spm_hnorm(A)
    % expected conditional entropy (ambiguity)
    %--------------------------------------------------------------------------
    A     = spm_norm(A);
    A     = full(sum(A.*spm_log(A),1));
    if ~any(A(:))
        A = false;
    end
end

function [M,MDP] = spm_MDP_get_M(MDP,T,Ng)
    % FORMAT [M,MDP] = spm_MDP_get_M(MDP,T,Ng)
    % returns an update matrix for multiple models
    % MDP(m) - structure array of m MPDs
    % T      - number of trials or updates
    % Ng(m)  - number of output modalities for m-th MDP
    %
    % M      - update matrix for multiple models
    % MDP(m) - structure array of m MPDs
    %
    % In some applications, the outcomes are generated by a particular model
    % (to maximise free energy, based upon the posterior predictive density).
    % The generating model is specified in the matrix MDP(m).n, with a row for
    % each outcome modality, such that each row lists the index of the model
    % responsible for generating outcomes.
    %__________________________________________________________________________
    
    for m = 1:size(MDP,1)
    
        % check size of outcome generating agent, as specified by MDP(m).n
        %----------------------------------------------------------------------
        if ~isfield(MDP(m),'n')
            MDP(m).n = zeros(Ng(m),T);
        elseif isempty(MDP(m).n)
            MDP(m).n = zeros(Ng(m),T);
        end
        if size(MDP(m).n,1) < Ng(m)
            MDP(m).n = repmat(MDP(m).n(1,:),Ng(m),1);
        end
        if size(MDP(m).n,2) < T
            MDP(m).n = repmat(MDP(m).n(:,1),1,T);
        end
    
        % mode of generating model (most frequent over outcome modalities)
        %----------------------------------------------------------------------
        n(m,:) = mode(MDP(m).n.*(MDP(m).n > 0),1);
    
    end
    
    % reorder list of model indices for each update
    %--------------------------------------------------------------------------
    n     = mode(n,1);
    for t = 1:T
        if n(t) > 0
            M(t,:) = circshift((1:size(MDP,1)),[0 (1 - n(t))]);
        else
            M(t,:) = 1:size(MDP,1);
        end
    end
    
    
    return
end

function [G,P,F] = spm_forwards(O,P,A,B,C,H,K,W,I,t,T,N,m,id)
    % deep tree search over policies or paths
    %--------------------------------------------------------------------------
    % FORMAT [G,Q,F] = spm_forwards(O,P,A,B,C,H,K,W,I,t,T,N,m)
    % O{m,g,t} - cell array of outcome probabilities for modality g
    % P{m,f,t} - cell array of priors over states
    % A{m,g}   - likelihood mappings from hidden states
    % B{m,f,k} - belief propagators (policy-dependent probability transitions)
    % C{m,g}   - priors over outcomes (cost or constraint)
    % H{m,f}   - priors over final states
    % K{m,g}   - likelihood ambiguity
    % W{m,g}   - likelihood novelty
    % I{m,f,k} - transition prior novelty
    %
    % t        - current time point
    % T        - time horizon
    % N        - policy horizon
    % m        - model or agent to update
    % id       - domains
    %
    % G(k)     - expected free energy over k policies
    % Q{m,f,t} - posterior over states
    % F        - variational free energy (negative or ELBO)
    %
    %  This subroutine performs a deep tree search over sequences of actions to
    %  evaluate the expected free energy over policies or paths. Crucially, it
    %  only searches likely policies under likely hidden states in the future.
    %  This search is sophisticated; in the sense that posterior beliefs are
    %  updated on the basis of future outcomes to evaluate the free energy
    %  under each outcome. The resulting  average is then accumulated to
    %  furnish a path integral of expected free energy for the next action.
    %  This routine operates recursively by updating predictive posteriors over
    %  hidden states and their most likely outcomes.
    %__________________________________________________________________________
    
    
    % Posterior over hidden states based on likelihood (A) and priors (P)
    %==========================================================================
    G        = zeros(size(B,3),1);                    % log priors over actions
    
    % variational (Bayesian) belief updating and free energy (ELBO)
    %--------------------------------------------------------------------------
    [Q,F]    = spm_VBX(O(m,:,t),P(m,:,t),A(m,:),id{m});
    P(m,:,t) = Q;
    
    
    % terminate search at time horizon
    %--------------------------------------------------------------------------
    if t == T || numel(G) == 1, return, end
    
    % Constraints on final state
    %==========================================================================
    R     = spm_cost(A(m,:),B(m,:,:),C(m,:),H(m,:),Q,(T - t),id{m});
    
    
    % Expected free energy of subsequent action
    %==========================================================================
    for k = 1:size(B,3)                               % search over actions
    
        % (negative) expected free energy
        %----------------------------------------------------------------------
        for f = 1:size(B,2)
    
            % predictive posterior
            %------------------------------------------------------------------
            Q{f,k} = B{m,f,k}*P{m,f,t};
    
            % G(k): risk over states
            %------------------------------------------------------------------
            G(k) = G(k) - Q{f,k}'*(spm_log(Q{f,k}) - R{f});
    
            % expected information gain (prior parameters: i.e., novelty)
            %------------------------------------------------------------------
            if any(I{m,f,k}(:))
                G(k) = G(k) + P{m,f,t}'*I{1,f,k}*Q{f,k};
            end
    
        end
    
        for g  = 1:size(A,2)                          % for all modalities
    
            % domain of A{g}
            %------------------------------------------------------------------
            j    = id{m}.A{g};
    
            % predictive posterior and prior over outcomes
            %------------------------------------------------------------------
            qo   = spm_dot(A{m,g},Q(j,k));            % predictive outcomes
            po   = C{m,g};                            % predictive log prior
    
            % G(k): risk over outomes
            %-----------------------------------------------------------------
            G(k) = G(k) - qo'*(spm_log(qo) - po);
    
            % G(k): ambiguity
            %------------------------------------------------------------------
            if any(K{m,g}(:))
                G(k) = G(k) + spm_dot(K{m,g},Q(j,k));
            end
    
            % expected information gain (likelihood parameters: i.e., novelty)
            %------------------------------------------------------------------
            if any(W{m,g}(:))
                G(k) = G(k) + qo'*spm_dot(W{m,g},Q(j,k));
            end
    
        end
    end
    
    
    % deep (recursive) search over action sequences ( i.e., paths)
    %==========================================================================
    if t < N
    
        % probability over action (terminating search at a suitable threshold)
        %----------------------------------------------------------------------
        u     = spm_softmax(G);
        k     = u > max(u)/16;
        u(~k) = 0;
        G(~k) = max(G) - 512;
    
        % otherwise, accumulate the path integral of expected free energy
        %----------------------------------------------------------------------
        for k = 1:size(B,3)                          % search over actions
            if u(k)                                  % evaluate plausible paths
    
                %  evaluate expected free energy for plausible hidden states
                %--------------------------------------------------------------
                q     = spm_vec(spm_cross(Q(:,k)));  % vectorise posterior
                j     = find(q > max(q)/16);
                if length(j) > 16
                    [q,j] = sort(q,'descend');
                    j     = j(1:16);
                end
    
                for i = j(:)'
    
                    % outcome probabilities under hidden state (i)
                    %----------------------------------------------------------
                    for g = 1:size(A,2)
                        O{m,g,t + 1} = A{m,g}(:,i);
                    end
    
                    % prior over subsequent action under this hidden state
                    %----------------------------------------------------------
                    P(m,:,t + 1) = Q(:,k);
                    E      = spm_forwards(O,P,A,B,C,H,K,W,I,t + 1,T,N,m,id);
    
                    % expected free energy marginalised over subsequent action
                    %----------------------------------------------------------
                    EFE(i) = spm_softmax(E)'*E;
    
                end
    
                % accumulate expected free energy marginalised over states
                %--------------------------------------------------------------
                G(k) = G(k) + EFE(j)*q(j);
    
            end % plausible paths
    
        end % search over actions
    
    
    end
end


function [Q,P,F] = spm_backwards(O,P,Q,D,E,a,b,U,m,id)
    % Backwards smoothing to evaluate posterior over initial states
    %--------------------------------------------------------------------------
    % O{m,g,t} - cell array of outcome probabilities for modality g
    % P{m,k,t} - cell array of posteriors over paths
    % Q{m,f,t} - cell array of posteriors over states
    % D{m,f}   - cell array of priors over initial states
    % E{m,k}   - belief propagators (action dependent probability transitions)
    % a{m,g}   - likelihood tensor  (Dirichlet parameters)
    % b{m,f}   - belief propagators (Dirichlet parameters)
    % U{f}     - controllable factors
    % m        - agent or model
    %
    % F        - Negative free energy (states, paths and parameters) ELBO
    %
    %  This subroutine performs Bayesian smoothing in the sense of a replay
    %  using variational iterations to optimise posteriors over states, paths
    %  and parameters, given the outcomes over an epoch. It effectively
    %  implements the prior constraint that certain random variables (i.e., the
    %  paths of uncontrollable factors and parameters) do not change with time
    %__________________________________________________________________________
    
    %  (iterative) variational scheme
    %==========================================================================
    tr    = @(A) pagetranspose(A);
    T     = size(Q,3);
    
    % variational iterations
    %--------------------------------------------------------------------------
    Z     = -Inf;
    for v = 1:16
    
        % initialise posterior Dirichlet parameters
        %----------------------------------------------------------------------
        F    = zeros(1,T);                   % variational free energy (ELBO)
        qa   = a(m,:);
        qb   = b(m,:);
    
        % acccumulate posterior Dirichlet parameters
        %======================================================================
        for t = 1:T
    
            % likelihood mapping from hidden states to outcomes: a
            %------------------------------------------------------------------
            for g = 1:numel(qa)
                j     = id{m}.A{g};
                qa{g} = qa{g} + spm_cross(O{m,g,t},Q{m,j,t});
                qa{g} = qa{g}.*(a{m,g} > 0);
            end
    
            % mapping from hidden states to hidden states: b(u)
            %------------------------------------------------------------------
            if t < T
                for f = 1:numel(qb)
                    qb{f} = qb{f} + spm_cross(spm_cross(Q{m,f,t + 1},Q{m,f,t}),P{m,f,t});
                    qb{f} = qb{f}.*(b{m,f} > 0);
                end
            end
    
        end
    
        % inference (Bayesian filtering)
        %======================================================================
        for t = 1:T
    
            % accumulate likelihoods over modalities
            %------------------------------------------------------------------
            L     = 0;
            for g = 1:numel(qa)
                j  = id{m}.A{g};
                LL = spm_dot(spm_norm(qa{g}),O{m,g,t});
                LL = spm_log(LL);
    
                k  = ones(1,8); k(j) = size(LL,1:numel(j));
                L  = plus(L, reshape(LL,k));
            end
    
            % preclude numerical overflow of log likelihood
            %------------------------------------------------------------------
            L = max(L, max(L(:)) - 16);
            
            % posterior over hidden states
            %------------------------------------------------------------------
            for f = 1:numel(qb)
    
                % log likelihood
                %--------------------------------------------------------------
                LL   = spm_vec(spm_dot(L,Q(m,:,t),f));
    
                % log prior: smoothing
                %--------------------------------------------------------------
                LP   = 0;
                if t == 1
                    LP = LP + spm_log(D{m,f});
                end
                if t < T
                    LP = LP + spm_dot(spm_psi(tr(qb{f})),P(m,f,t))*Q{m,f,t + 1};
                end
                if t > 1
                    LP = LP + spm_dot(spm_psi(qb{f}),P(m,f,t - 1))*Q{m,f,t - 1};
                end
    
                % posterior
                %--------------------------------------------------------------
                Q{m,f,t} = spm_softmax(LL + LP);
    
                % ELBO free energy of states (accuracy and complexity)
                %--------------------------------------------------------------
                F(t)  = F(t) + Q{m,f,t}'*(LL + LP - spm_log(Q{m,f,t}));
    
            end
    
        end
    
        % beliefs about paths
        %======================================================================
        for f = 1:numel(qb)
    
            % beliefs about (changing) paths
            %------------------------------------------------------------------
            if U{m}(f)
    
                for t = 2:T
    
                    % log likelihood of control states
                    %----------------------------------------------------------
                    LL = spm_dot(spm_dot(spm_psi(qb{f}),Q{m,f,t}),Q{m,f,t - 1});
    
                    % prior over control state
                    %----------------------------------------------------------
                    LP = spm_log(P{m,f,t - 1});
    
                    % posterior over control states
                    %----------------------------------------------------------
                    P{m,f,t - 1} = spm_softmax(LL + LP);
    
                    % ELBO free energy of paths (complexity)
                    %----------------------------------------------------------
                    F(t)  = F(t) + P{m,f,t - 1}'*(LL + LP - spm_log(P{m,f,t - 1}));
    
                end
    
            else  % beliefs about (unchanging) paths
    
                % accumulate log likelihood
                %--------------------------------------------------------------
                LL    = 0;
                for t = 2:T
                    LL = LL + spm_dot(spm_dot(spm_psi(qb{f}),Q{m,f,t}),Q{m,f,t - 1});
                end
    
                % prior over control state
                %--------------------------------------------------------------
                LP = spm_log(E{m,f});
    
                % posterior over control states
                %--------------------------------------------------------------
                PP    = spm_softmax(LL + LP);
                for t = 1:T
                    P{m,f,t} = PP;
                end
    
                % ELBO free energy of paths (complexity)
                %--------------------------------------------------------------
                F(t)  = F(t) + PP'*(LL + LP - spm_log(PP));
    
            end
        end
    
        % convergence
        %======================================================================
        dF = sum(F) - Z;
    
        % checks on ELBO
        %------------------------------------------------------------------------
        if sum(F) > 0
            warning('positive ELBO in spm_backwards')
        end
        if dF < 1/128
            break
        else
            Z = sum(F);
        end
    
    end
    
    
    return
end

function H = spm_cost(A,B,C,H,Q,N,id)
    % Inductive inference from final to next state
    % FORMAT P = spm_cost(A,B,C,H,Q,N,id)
    %--------------------------------------------------------------------------
    % A{1,g}   - likelihood mappings from hidden states
    % B{1,f,k} - belief propagators (policy-dependent probability transitions)
    % C{1,g}   - priors over outcomes (cost or constraint)
    % H{1,f}   - cell array of priors over final state
    % Q{1,f}   - cell array of posteriors over states
    %
    % N      - induction depth
    % 
    % P{f}   - cell array of priors over current state
    %
    % This subroutine estimates the next cost in terms of surprisal over future
    % states based upon backwards induction of a simple sort; i.e., using
    % backwards propagators to identify paths of least action.
    %__________________________________________________________________________
    
    
    
    % Preliminary checks (no priors over final states)
    %==========================================================================
    d     = true;
    for f = 1:numel(H)
        d = d & ~any(diff(H{f}));
    end
    if d, return, end
    
    % Threshold transition probabilities
    %--------------------------------------------------------------------------
    u     = 1/16;
    for f = 1:numel(H)
        b{f}  = 0;
        Ns(f) = numel(H{f});
        for k = 1:size(B,3)
            b{f} = b{f} + B{1,f,k};
        end
        b{f} = gt(b{f}, max(b{f})*u);
        h{f} = gt(H{f}, max(H{f})*u);
    end
    
    % Kronecker tensor product
    %--------------------------------------------------------------------------
    Bf    = 1;
    Pf    = 1;
    Qf    = 1;
    for f = 1:size(B,2)
        Bf = spm_kron(Bf,b{f});
        Pf = spm_kron(Pf,h{f});
        Qf = spm_kron(Qf,Q{f});
    end
    
    % Expected cost in latent state space
    %==========================================================================
    Ns    = [Ns,1];
    L     = zeros(Ns);
    for g = 1:numel(A)
        j = id.A{g};
        U = spm_dot(A{g},C{g});
    
        k = ones(1,numel(Ns)); k(j) = size(U,1:numel(j));
        L = plus(L, reshape(U,k));
    end
    
    % Constraint
    %--------------------------------------------------------------------------
    L     = spm_vec(L);
    L     = L > (max(L) + log(u));
    Bf    = times(Bf,L);
    Bf    = logical(Bf);
    Pf    = logical(Pf);
    
    
    % Paths of least action
    %==========================================================================
    
    % backwards protocol
    %--------------------------------------------------------------------------
    for n = 1:N
        p = any(Bf(Pf(:,n),:),1)' & ~any(Pf,2);
        if any(p)
            Pf(:,n + 1) = p;
        else
            break
        end
    end
    
    % Find most likely point on path of least action
    %--------------------------------------------------------------------------
    G     = zeros(size(Pf,2),1);
    for n = 1:size(Pf,2)
        G(n) = G(n) + Qf'*Pf(:,n);
    end
    [d,n] = max(G);
    
    % log prior over next state
    %==========================================================================
    Pf    = full(Pf(:,max(n - 1,1)));
    
    % Marginal constraints
    %--------------------------------------------------------------------------
    Pf    = reshape(Pf,Ns);
    H     = spm_marginal(Pf);
    for f = 1:numel(H)
        H{f} = spm_log(spm_norm(double(H{f})));
    end
    
    
    return
end

function MDP = spm_MDP_update(MDP,OUT)
    % FORMAT MDP = spm_MDP_update(MDP,OUT)
    % moves Dirichlet parameters from OUT to MDP
    % MDP - structure array (new)
    % OUT - structure array (old)
    %__________________________________________________________________________
    
    % check for concentration parameters at this level
    %--------------------------------------------------------------------------
    try,  MDP.a = OUT.a; end
    try,  MDP.b = OUT.b; end
    try,  MDP.c = OUT.c; end
    try,  MDP.d = OUT.d; end
    try,  MDP.e = OUT.e; end
    
    % check for concentration parameters at nested levels
    %--------------------------------------------------------------------------
    try,  MDP.MDP(1).a = OUT.mdp(end).a; end
    try,  MDP.MDP(1).b = OUT.mdp(end).b; end
    try,  MDP.MDP(1).c = OUT.mdp(end).c; end
    try,  MDP.MDP(1).d = OUT.mdp(end).d; end
    try,  MDP.MDP(1).e = OUT.mdp(end).e; end
    
    return
end
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
    
    function r = spm_gamrnd(h,l)
    % samples from a gamma distribution
    % FORMAT r = spm_gamrnd(h,lambda)
    % h - shape parameter
    % l - scale parameter
    %
    % generates a random number from the gamma distribution with the shape
    % parameter h and the scale parameter l.
    %
    % e.g.: r = spm_gamrnd(1,2)
    % r = 
    %     7.1297
    %
    % see also: spm_Gpdf(x,h,l)
    %__________________________________________________________________________
    
    U = rand(10000,h);
    r = sum(-log(U),2)/l;
    
    return
    end
    
    function r = spm_drchrnd(a,n)
    % samples from a Dirichlet distribution
    % FORMAT r = spm_drchrnd(a,n)
    % a - Dirichlet paramters
    % l - scale parameter
    %
    % e.g.: A = spm_drchrnd([1 1 1 1], 3)
    % 
    % A =
    % 
    % 0.3889 0.1738 0.0866 0.3507
    % 0.0130 0.0874 0.6416 0.2579
    % 0.0251 0.0105 0.2716 0.6928
    %__________________________________________________________________________
    
    p = length(a);
    r = spm_gamrnd(repmat(a,n,1),1,n,p);
    r = r ./ repmat(sum(r,2),1,p);
    
    return
end